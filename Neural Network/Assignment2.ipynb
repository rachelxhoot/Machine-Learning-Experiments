{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二题：神经网络：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验内容：\n",
    "1. 学会梯度下降的基本思想\n",
    "2. 学会使用梯度下降求解线性回归\n",
    "3. 了解归一化处理的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig0.png\" width=300>\n",
    "\n",
    "我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。  \n",
    "我们记输入为$X \\in \\mathbb{R}^{n \\times m}$，输出为$Z \\in \\mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。  \n",
    "输入层到输出层的权重和偏置，我们记为$W \\in \\mathbb{R}^{m}$和$b \\in \\mathbb{R}$。  \n",
    "输出层没有激活函数，所以上面的神经网络的前向传播过程写为：\n",
    "\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "我们使用均方误差作为模型的损失函数\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "我们通过调整参数$W$和$b$来降低均方误差，或者说是**以降低均方误差为目标，学习参数$W$和参数$b$**。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。\n",
    "\n",
    "在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。\n",
    "\n",
    "在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数：\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。\n",
    "\n",
    "接下来我们会求解损失函数对参数的偏导数。\n",
    "\n",
    "损失函数MSE记为：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$\n",
    "\n",
    "其中，$Z \\in \\mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。\n",
    "\n",
    "我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘：\n",
    "\n",
    "**这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\\n",
    "&= - \\frac{2}{n} X^\\mathrm{T} (y - Z)\\\\\n",
    "&= \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "\\end{aligned}$$\n",
    "\n",
    "同理，求解损失函数对参数$b$的偏导数:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\\n",
    "&= - \\frac{2}{n} \\sum^n_{i=1}(y_i - Z_i)\\\\\n",
    "&= \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。**\n",
    "\n",
    "其中，$\\frac{\\partial \\mathrm{loss}}{\\partial W} \\in \\mathbb{R}^{m}$，$\\frac{\\partial \\mathrm{loss}}{\\partial b} \\in \\mathbb{R}$，求解得到的梯度的维度与参数一致。\n",
    "\n",
    "完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练神经网络的基本思路：\n",
    "\n",
    "1. 首先对参数进行初始化，对参数进行随机初始化（也就是取随机值）\n",
    "2. 将样本输入神经网络，计算神经网络预测值 $Z$\n",
    "3. 计算损失值MSE\n",
    "4. 通过 $Z$ 和 $y$ ，以及 $X$ ，计算参数的梯度\n",
    "5. 使用梯度下降更新参数\n",
    "6. 循环1-5步，**在反复迭代的过程中可以看到损失值不断减小的现象，如果没有下降说明出了问题**\n",
    "\n",
    "接下来我们来实现这个最简单的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用kaggle房价数据，选3列作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('data/kaggle_house_price_prediction/kaggle_hourse_price_train.csv')\n",
    "\n",
    "# 使用这3列作为特征\n",
    "features = ['LotArea', 'BsmtUnfSF', 'GarageArea']\n",
    "target = 'SalePrice'\n",
    "data = data[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40%做测试集，60%做训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(data[features], data[target], test_size = 0.4, random_state = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集876个样本，3个特征，测试集584个样本，3个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((876, 3), (876,), (584, 3), (584,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要初始化参数$W$和$b$，其中$W \\in \\mathbb{R}^m$，$b \\in \\mathbb{R}$，初始化的策略是将$W$初始化成一个随机数矩阵，参数$b$为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(m):\n",
    "    '''\n",
    "    参数初始化，将W初始化成一个随机向量，b是一个长度为1的向量\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m: int, 特征数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    W: np.ndarray, shape = (m, ), 参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, ), 参数b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 指定随机种子，这样生成的随机数就是固定的了，这样就可以与下面的测试样例进行比对\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    W = np.random.normal(size = (m, )) * 0.01\n",
    "    \n",
    "    b = np.zeros((1, ))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要完成输入矩阵$X$在神经网络中的计算，也就是完成 $Z = XW + b$ 的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    '''\n",
    "    前向传播，计算Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 完成Z = XW + b的计算\n",
    "    Z = np.dot(X,W) +b # YOUR CODE HERE\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.37377228144393\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = forward(trainX, Wt, bt)\n",
    "print(tmp.mean()) # -28.37377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来编写损失函数，我们以均方误差(MSE)作为损失函数，需要大家实现MSE的计算：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE，均方误差\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, )，预测值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float，损失值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 计算MSE\n",
    "    n=y_pred.shape[0]\n",
    "    loss = np.sum(np.square(y_pred-y_true)) /n# YOUR CODE HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39381033680.460075\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = mse(trainY, forward(trainX, Wt, bt))\n",
    "print(tmp) # 39381033680.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们要完成梯度的计算，也就是计算出损失函数对参数的偏导数的导数值：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} = \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} = \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Z, y_true):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n = len(y_true)\n",
    "    \n",
    "    # 计算W的梯度\n",
    "    dW = 2/n * np.dot(X.T,(Z-y_true)) # YOUR CODE HERE\n",
    "    \n",
    "    # 计算b的梯度\n",
    "    db = 2/n * np.sum(Z-y_true)            # YOUR CODE HERE\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "-1532030241.2528899\n",
      "-364308.5557637409\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print(dWt.shape) # (3,)\n",
    "print(dWt.mean()) # -1532030241.25\n",
    "print(dbt.mean()) # -364308.555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分需要实现梯度下降的函数\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(dW, db, W, b, learning_rate):\n",
    "    '''\n",
    "    梯度下降，参数更新，不需要返回值，W和b实际上是以引用的形式传入到函数内部，\n",
    "    函数内改变W和b会直接影响到它们本身，所以不需要返回值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 更新W\n",
    "    W -= learning_rate * dW\n",
    "    \n",
    "    # 更新b\n",
    "    b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "(3,)\n",
      "15320302.416581342\n",
      "3643.0855576374092\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 3643.08555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成整个参数更新的过程，先计算梯度，再更新参数，将compute_gradient和update组装在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Z, y_true, W, b, learning_rate):\n",
    "    '''\n",
    "    使用compute_gradient和update函数，先计算梯度，再更新参数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算参数的梯度\n",
    "    dW, db = compute_gradient(X,Z,y_true) # YOUR CODE HERE\n",
    "    \n",
    "    # 更新参数\n",
    "    update(dW,db,W,b,learning_rate)# YOUR CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "(3,)\n",
      "15320302.416581342\n",
      "3643.0855576374092\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "backward(trainX, Zt, trainY, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 3643.08555764"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, W, b, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播，更新参数\n",
    "    同时记录训练集和测试集上的损失值，后面画图用。然后循环往复，直到达到最大迭代次数epochs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, ), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, )，测试集的标记\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，参数b\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # 这里我们要将神经网络的输出值保存起来，因为后面反向传播的时候需要这个值\n",
    "        Z = forward(trainX, W, b)\n",
    "        \n",
    "        # 计算训练集的损失值\n",
    "        training_loss = mse(trainY, Z)\n",
    "        \n",
    "        # 计算测试集的损失值        \n",
    "        testing_loss = mse(testY, forward(testX, W, b))\n",
    "        \n",
    "        # 将损失值存起来\n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        # 打印损失值，debug用\n",
    "        if verbose:\n",
    "            print('epoch %s training loss: %s'%(epoch+1, training_loss))\n",
    "            print('epoch %s testing loss: %s'%(epoch+1, testing_loss))\n",
    "            print()\n",
    "        \n",
    "        # 反向传播，参数更新\n",
    "        backward(trainX, Z, trainY, W, b, learning_rate)\n",
    "        \n",
    "    return training_loss_list, testing_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "[39381033680.460075, 3.390230782482133e+23]\n",
      "[38555252685.09387, 4.1516070231815916e+23]\n",
      "-57055790600891.0\n",
      "-8824267814.591059\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean())          # 0.00405243937693\n",
    "print(bt.mean())          # 0.0\n",
    "\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, Wt, bt, 2, learning_rate = 0.01, verbose = False)\n",
    "\n",
    "print(training_loss_list) # [39381033680.460075, 3.3902307664083424e+23]\n",
    "print(testing_loss_list)  # [38555252685.093872, 4.1516070070405267e+23]\n",
    "print(Wt.mean())          # -5.70557904608e+13\n",
    "print(bt.mean())          # -8824267814.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 检查\n",
    "\n",
    "编写一个绘制损失值变化曲线的函数\n",
    "\n",
    "一般我们通过绘制损失函数的变化曲线来判断模型的拟合状态。\n",
    "\n",
    "一般来说，随着迭代轮数的增加，训练集的loss在下降，而测试集的loss在上升，这说明我们正在不断地让模型在训练集上表现得越来越好，在测试集上表现得越来越糟糕，这就是过拟合的体现。  \n",
    "\n",
    "如果训练集loss和测试集loss共同下降，这就是我们想要的结果，说明模型正在很好的学习。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这些函数就是完成整个神经网络需要的函数了\n",
    "\n",
    "|函数名|功能|\n",
    "|-|-|\n",
    "|initialize | 参数初始化|\n",
    "|forward | 给定数据，计算神经网络的输出值|\n",
    "|mse | 给定真值，计算神经网络的预测值与真值之间的差距|\n",
    "|backward | 计算参数的梯度，并实现参数的更新|\n",
    "|compute_gradient | 计算参数的梯度|\n",
    "|update | 参数的更新|\n",
    "|backward | 计算参数梯度，并且更新参数|\n",
    "|train | 训练神经网络|\n",
    "|plot_loss_curve | 绘制损失函数的变化曲线|\n",
    "\n",
    "我们使用参数初始化函数和训练函数，完成神经网络的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss: 39381033680.460075\n",
      "epoch 1 testing loss: 38555252685.09387\n",
      "\n",
      "epoch 2 training loss: 3.390230782482133e+23\n",
      "epoch 2 testing loss: 4.1516070231815916e+23\n",
      "\n",
      "epoch 3 training loss: 5.055503104347344e+36\n",
      "epoch 3 testing loss: 6.193449607112223e+36\n",
      "\n",
      "epoch 4 training loss: 7.538764120634865e+49\n",
      "epoch 4 testing loss: 9.235676243151318e+49\n",
      "\n",
      "epoch 5 training loss: 1.1241801912459202e+63\n",
      "epoch 5 testing loss: 1.3772236581164481e+63\n",
      "\n",
      "epoch 6 training loss: 1.6763770323182435e+76\n",
      "epoch 6 testing loss: 2.0537153445830472e+76\n",
      "\n",
      "epoch 7 training loss: 2.499812731417686e+89\n",
      "epoch 7 testing loss: 3.0624994652885273e+89\n",
      "\n",
      "epoch 8 training loss: 3.727719702480175e+102\n",
      "epoch 8 testing loss: 4.566797925345696e+102\n",
      "\n",
      "epoch 9 training loss: 5.558774065599014e+115\n",
      "epoch 9 testing loss: 6.810007161577364e+115\n",
      "\n",
      "epoch 10 training loss: 8.289241568194472e+128\n",
      "epoch 10 testing loss: 1.0155079839059102e+129\n",
      "\n",
      "epoch 11 training loss: 1.236091356925456e+142\n",
      "epoch 11 testing loss: 1.5143250820573032e+142\n",
      "\n",
      "epoch 12 training loss: 1.8432589159041973e+155\n",
      "epoch 12 testing loss: 2.2581609307764234e+155\n",
      "\n",
      "epoch 13 training loss: 2.748666926618768e+168\n",
      "epoch 13 testing loss: 3.367368638150892e+168\n",
      "\n",
      "epoch 14 training loss: 4.098810974573112e+181\n",
      "epoch 14 testing loss: 5.021418708764681e+181\n",
      "\n",
      "epoch 15 training loss: 6.112145215771043e+194\n",
      "epoch 15 testing loss: 7.48793748420071e+194\n",
      "\n",
      "epoch 16 training loss: 9.114428396533624e+207\n",
      "epoch 16 testing loss: 1.1166009253407111e+208\n",
      "\n",
      "epoch 17 training loss: 1.3591431823508949e+221\n",
      "epoch 17 testing loss: 1.6650748341615208e+221\n",
      "\n",
      "epoch 18 training loss: 2.0267537466567478e+234\n",
      "epoch 18 testing loss: 2.4829588982402504e+234\n",
      "\n",
      "epoch 19 training loss: 3.0222943417058274e+247\n",
      "epoch 19 testing loss: 3.702587273475295e+247\n",
      "\n",
      "epoch 20 training loss: 4.5068440618277324e+260\n",
      "epoch 20 testing loss: 5.521296597949055e+260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特征数m\n",
    "m = trainX.shape[1]\n",
    "\n",
    "# 参数初始化\n",
    "W, b = initialize(m)\n",
    "\n",
    "# 训练20轮，学习率为0.01\n",
    "training_loss_list, testing_loss_list = train(trainX, trainY, testX, testY, W, b, 20, learning_rate = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失值的变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印损失的信息我们可以看到损失值持续上升，这就说明哪里出了问题。但是如果所有的测试样例都通过了，就说明我们的实现是没有问题的。运行下面的测试样例，观察哪里出了问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, W: [-0.00348894  0.00983703  0.00580923]\n",
      "epoch 0, b: [0.]\n",
      "\n",
      "dWt: [-4.18172940e+09 -2.19880296e+08 -1.94481031e+08]\n",
      "db: -364308.5557637409\n",
      "\n",
      "epoch 1, W: [41817293.96016916  2198802.97412493  1944810.31544993]\n",
      "epoch 1, b: [3643.08555764]\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print('epoch 0, W:', Wt)  # [-0.00348894  0.00983703  0.00580923]\n",
    "print('epoch 0, b:', bt)  # [ 0.]\n",
    "print()\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print('dWt:', dWt) # [ -4.18172940e+09  -2.19880296e+08  -1.94481031e+08]\n",
    "print('db:', dbt) # -364308.555764\n",
    "print()\n",
    "\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "print('epoch 1, W:', Wt)  # [ 41817293.96016914   2198802.97412493   1944810.31544994]\n",
    "print('epoch 1, b:', bt)  # [ 3643.08555764]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们最开始的参数都是在 $10^{-3}$ 这个数量级上，而第一轮迭代时计算出的梯度的数量级在 $10^8$ 左右，这就导致使用梯度下降更新的时候，让参数变成了 $10^6$ 这个数量级左右（学习率为0.01）。产生这样的问题的主要原因是：我们的原始数据 $X$ 没有经过适当的处理，直接扔到了神经网络中进行训练，导致在计算梯度时，由于 $X$ 的数量级过大，导致梯度的数量级变大，在参数更新时使得参数的数量级不断上升，导致参数无法收敛。\n",
    "\n",
    "解决的方法也很简单，对参数进行归一化处理，将其标准化，使均值为0，缩放到 $[-1, 1]$附近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化处理和第一题一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stand = StandardScaler()\n",
    "trainX_normalized = stand.fit_transform(trainX)\n",
    "testX_normalized = stand.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新训练模型，这次我们迭代40轮，学习率设置为0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainX.shape[1]\n",
    "W, b = initialize(m)\n",
    "training_loss_list, testing_loss_list = train(trainX_normalized, trainY, testX_normalized, testY, W, b, 40, learning_rate = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集上的MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60305.852679101554"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = forward(testX_normalized, W, b)\n",
    "mse(testY, prediction) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
